
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>HumanBench</title>
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.11.2/css/all.css">
  <!-- Bootstrap core CSS -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet">
  <!-- OpenMMLab Data Protocol -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" rel="stylesheet">
  <!-- Your custom styles (optional) -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/jstree/3.3.9/themes/default/style.min.css" rel="stylesheet">

</head>

<body>

  <!--Main Navigation-->
  <header>

    <!-- Navbar -->
    <nav class="navbar fixed-top navbar-expand-lg navbar-light white scrolling-navbar">
      <div class="container">

        <!-- Brand -->
        <a class="navbar-brand waves-effect" href="#">
          <strong class="red-text">HumanBench</strong>
        </a>

        <!-- Collapse -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
          aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <!-- Links -->
        <div class="collapse navbar-collapse" id="navbarSupportedContent">

          <!-- Left -->
          <ul class="navbar-nav mr-auto">
            <li class="nav-item active">
              <a class="nav-link waves-effect" href="#">Home
                <span class="sr-only">(current)</span>
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link waves-effect" href="#video">Video</a>
            </li>
            <li class="nav-item">
              <a class="nav-link waves-effect" href="#dataset">Dataset</a>
            </li>
            <li class="nav-item">
              <a class="nav-link waves-effect" href="#paper">Paper</a>
            </li>
          </ul>

        </div>

      </div>
    </nav>
    <!-- Navbar -->

  </header>
  <!--Main Navigation-->

  <!--Main layout-->
  <main class="mt-5 pt-5">
    <div class="container">
      <!--Section: Jumbotron-->
      <section class="card peach-gradient wow fadeIn" id="intro">
        <!-- Content -->
        <div class="card-body text-white text-center py-5 px-5 my-5">
          <img src="img/cover.png"
            class="img-fluid" alt="img-404">
          <h1 class="mb-4">
          </br>
            <strong>  
              HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining</strong>
            </h1>
          <p>
            <!-- <strong>A versatile benchmark for comprehensive forgery analysis. </strong> -->
            Shixiang Tang, 
            Cheng Chen, 
            Meilin Chen, 
            Qingsong Xie,
            Yizhou Wang, 
            Yuanzheng Ci,
            LEI BAI, 
            Feng Zhu, 
            Haiyang Yang, 
            Li Yi, 
            Rui Zhao, 
            Wanli Ouyang 
          </p>
          
          <a target="_blank" href="https://gitlab.bj.sensetime.com/vitruvian/vitruvian-multitask/-/tree/chencheng/benchmark_v100_32g_sh1984" class="btn btn-outline-white btn-lg">code
            <!-- <i class="fas fa-graduation-cap ml-2"></i> -->
          </a>


          <h1 class="mb-4">
            </br>
            <strong>  
              UniHCP: A Unified Model for Human-Centric Perceptions</strong>
          </h1>
          <p>
            <!-- <strong>A versatile benchmark for comprehensive forgery analysis. </strong> -->
            Yuanzheng Ci, Yizhou Wang, Meilin Chen, Shixiang Tang, Feng Zhu, Lei Bai, Fengwei Yu, Rui Zhao, Wanli Ouyang
          </p>
          
          <a target="_blank" href="https://gitlab.bj.sensetime.com/vitruvian/vitruvian-multitask/-/tree/vit_base" class="btn btn-outline-white btn-lg">code
            <!-- <i class="fas fa-graduation-cap ml-2"></i> -->
          </a>

        </div>
        <!-- Content -->
      </section>
      <!--Section: Jumbotron-->
           
              
      <section class="pt-5" id='#Video'>
        <div class="wow fadeIn">
          <h2 class="h1 text-left mb-5">Video</h2>
        </div>
        <div class="filter filter-basic">
          <hr class="mb-5">
          <div class="filter-gallery">
            <div class="filter-item" data-category="place">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/e8XIL3Di2Y8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <hr class="mb-5">
            </div>
          </div>
        </div>
      </section> 


      <!--Section: Cards-->
      <section class="pt-5" id='#download'>
        <!-- Heading & Description -->
        <div class="wow fadeIn">
          <!--Section heading-->
          <h2 class="h1 text-left mb-5">Dataset</h2>
        </div>
        <!-- Heading & Description -->

        <div class="filter filter-basic">
          <hr class="mb-5">
          <div class="filter-gallery">



      <div class="filter-item" data-category="place">
          <div class="row mt-3 wow fadeIn">
          <!--Grid column-->
          <div class="card-body text-white text-center py-4 px-4 my-4">
              <!--Featured image-->
              <div class="view overlay rounded z-depth-1">
              <img src="img/dataset.png"
                  class="img-fluid" alt="img-404">
              </div>
          </div>
          <!--Grid column-->

          <!--Grid column-->
          <div class="col-lg-7 col-xl-7 ml-xl-4 mb-4">
              <p class="grey-text">
              Download dataset here:   <br>
              </p>

              <a href="" class="btn btn-sm btn-outline-dark"> Full Traing Set
                  <i class="fas fa-download ml-2"></i>
              </a>
                    
              <a href="" class="btn btn-sm btn-outline-dark"> Full Test Set
                  <i class="fas fa-download ml-2"></i>
              </a>  
            
              <a href="" class="btn btn-sm btn-outline-dark"> Sub Traing Set
                  <i class="fas fa-download ml-2"></i>
              </a>  
              
              <a href="" class="btn btn-sm btn-outline-dark"> Sub Test Set
                  <i class="fas fa-download ml-2"></i>
              </a> 
            
          </div>
          <!--Grid column-->

          </div>
          <!--Grid row-->

          <hr class="mb-5">
          </div>>  HumanBench collects 39 publicly available datasets of 5 human-centric tasks, including person ReID, human parsing, pose estimation, pedestrian detection, and pedestrian attribute. The existing distribution of datasets includes large numbers of human-centric cropped images in ReID, video frames in person pose estimation, and human parsing. In particular, to avoid information reduction, we select a single frame from every 8 video frames. Particularly, except for using training images in all datasets, we also use all/partial test images in some datasets. Specifically, for the person Reid task, we use all test images in LaST and partial test images in the PRCC dataset; for the human parsing task, we only use train images and publicly released images in DeepFashion(~half). For the pedestrian detection dataset, we remove the images in which there is no person. For the pose estimation datasets, we only use train images. For the pedestrian attribute recognition dataset, we only use partial test images in the UAV-Human dataset and do not contain test images in other pedestrian attribute recognition datasets. All the images in the pretraining dataset have been de-duplicated with the testing datasets to be a meaningful benchmark of our HumanBench.
          
          </div>>  Due to the significant computational cost when we pretrain the model on the full dataset, we select 17 subsets from 39 full datasets for ablation study, which contains 1,270,186 images as a similar number with ImageNet-1K. For the person ReID task, we select widely-used Market1501 and CUHK03 datasets, and the clothes-changing ReID dataset PRCC, forming a total of 105,708 images. For the human parsing task, we select widely used Human3.6M, LIP, CIHP, VIP datasets and one clothes parsing dataset ModaNet, with a total of 192,124 images. For the pose estimation task, we select widely-used COCO, AIC, and PoseTrack datasets with a total of 748,812 images. For the attribute task, we select PA-100K, RAPv2, HARDHC, and UAV-Human datasets with a total of 208,542. Due to the significant resource cost, for the pedestrian detection task, we only select one widely used dataset CrowdHuman.
          <hr class="mb-5">
      </div>
              </div><!-- filter-gallary -->

              </div><!-- filter -->
            </section>
            <!--Section: Cards-->
    </div>
  </main>
  <footer class="page-footer text-center font-small mdb-color darken-2 mt-4 wow fadeIn">
    
<div class="footer-copyright py-3">
    SenseTime research, 2022. Powered by
    <a href="https://mdbootstrap.com/education/bootstrap/" target="_blank"> MDBootstrap </a>.
    <!--Last updated at 2020.05-->
</div>
  </footer>
  <!--/.Footer-->

  <!-- SCRIPTS -->
  <!-- JQuery -->
  <script type="text/javascript" src="js/jquery-3.4.1.min.js"></script>
  <!-- Bootstrap tooltips -->
  <script type="text/javascript" src="js/popper.min.js"></script>
  <!-- Bootstrap core JavaScript -->
  <script type="text/javascript" src="js/bootstrap.min.js"></script>
  <!-- MDB core JavaScript -->
  <script type="text/javascript" src="js/mdb.min.js"></script>
  <!-- Initializations -->
  <script type="text/javascript">
    // Animations initialization
    new WOW().init();

  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/web-animations/2.3.1/web-animations.min.js"></script>
  <script src="js/filter.js?ver=20200529"></script>

  <script type="text/javascript">
    init_filter('.filter-basic');

  </script>

</body>

</html>
